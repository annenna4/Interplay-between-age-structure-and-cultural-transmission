{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "approximate-frederick",
   "metadata": {},
   "source": [
    "# Classifying Population Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "verbal-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/dataset.py\n",
    "%run ../src/augmentation.py\n",
    "%run ../src/simulation.py\n",
    "%run ../src/nets.py\n",
    "%run ../src/trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "graduate-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.8G\r\n",
      "drwxrwxr-x 2 folgertk folgertk 4.0K Mar 30 19:03 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxrwxr-x 6 folgertk folgertk 4.0K Mar 29 19:39 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-rw-r-- 1 folgertk folgertk  77M Mar 29 18:18 20210329181800.ids.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk  120 Mar 29 18:18 20210329181800.params.json\r\n",
      "-rw-rw-r-- 1 folgertk folgertk 268M Mar 29 18:18 20210329181800.samples.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk 115M Mar 29 18:18 20210329181800.theta.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk  77M Mar 29 19:55 20210329195537.ids.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk  120 Mar 29 19:55 20210329195537.params.json\r\n",
      "-rw-rw-r-- 1 folgertk folgertk 268M Mar 29 19:55 20210329195537.samples.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk 115M Mar 29 19:55 20210329195537.theta.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk  77M Mar 29 20:22 20210329202205.ids.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk  120 Mar 29 20:22 20210329202205.params.json\r\n",
      "-rw-rw-r-- 1 folgertk folgertk 268M Mar 29 20:22 20210329202205.samples.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk 115M Mar 29 20:22 20210329202205.theta.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk  77M Mar 30 19:03 20210330190331.ids.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk  120 Mar 30 19:03 20210330190331.params.json\r\n",
      "-rw-rw-r-- 1 folgertk folgertk 268M Mar 30 19:03 20210330190331.samples.npy\r\n",
      "-rw-rw-r-- 1 folgertk folgertk 115M Mar 30 19:03 20210330190331.theta.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls -lah ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "motivated-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PresimulatedDataset.load(\"../data/20210330190331\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cosmetic-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TimeseriesClassifier(MultiLayerPerceptron(128), ResNet(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "subjective-oracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: train loss = 0.52, val loss = 0.51, AUC = 0.83\u001b[32m ++\u001b[0m\n",
      "Epoch  2: train loss = 0.28, val loss = 0.39, AUC = 0.93\u001b[32m ++\u001b[0m\n",
      "Epoch  3: train loss = 0.20, val loss = 0.20, AUC = 0.97\u001b[32m ++\u001b[0m\n",
      "Epoch  4: train loss = 0.18, val loss = 0.17, AUC = 0.98\u001b[32m ++\u001b[0m\n",
      "Epoch  5: train loss = 0.17, val loss = 0.19, AUC = 0.98\u001b[31m --\u001b[0m\n",
      "Epoch  6: train loss = 0.16, val loss = 0.19, AUC = 0.98\u001b[31m --\u001b[0m\n",
      "Epoch  7: train loss = 0.16, val loss = 0.19, AUC = 0.98\u001b[31m --\u001b[0m\n",
      "Epoch  8: train loss = 0.17, val loss = 0.17, AUC = 0.98\u001b[31m --\u001b[0m\n",
      "Epoch  9: train loss = 0.15, val loss = 0.33, AUC = 0.95\u001b[31m --\u001b[0m\n",
      "Epoch 10: train loss = 0.16, val loss = 0.20, AUC = 0.98\u001b[31m --\u001b[0m\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 11: train loss = 0.15, val loss = 0.13, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 12: train loss = 0.13, val loss = 0.13, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 13: train loss = 0.12, val loss = 0.13, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 14: train loss = 0.12, val loss = 0.12, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 15: train loss = 0.12, val loss = 0.12, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 16: train loss = 0.12, val loss = 0.12, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 17: train loss = 0.12, val loss = 0.12, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 18: train loss = 0.12, val loss = 0.12, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 19: train loss = 0.11, val loss = 0.12, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 20: train loss = 0.11, val loss = 0.11, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 21: train loss = 0.11, val loss = 0.11, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 22: train loss = 0.11, val loss = 0.11, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 23: train loss = 0.11, val loss = 0.11, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 24: train loss = 0.10, val loss = 0.11, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 25: train loss = 0.10, val loss = 0.10, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 26: train loss = 0.10, val loss = 0.10, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 27: train loss = 0.10, val loss = 0.10, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 28: train loss = 0.10, val loss = 0.10, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 29: train loss = 0.09, val loss = 0.09, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 30: train loss = 0.09, val loss = 0.09, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 31: train loss = 0.09, val loss = 0.09, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 32: train loss = 0.09, val loss = 0.09, AUC = 0.99\u001b[32m ++\u001b[0m\n",
      "Epoch 33: train loss = 0.08, val loss = 0.09, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 34: train loss = 0.08, val loss = 0.09, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 35: train loss = 0.08, val loss = 0.09, AUC = 1.00\u001b[31m --\u001b[0m\n",
      "Epoch 36: train loss = 0.08, val loss = 0.09, AUC = 0.99\u001b[31m --\u001b[0m\n",
      "Epoch 37: train loss = 0.08, val loss = 0.09, AUC = 0.99\u001b[31m --\u001b[0m\n",
      "Epoch 38: train loss = 0.07, val loss = 0.09, AUC = 0.99\u001b[31m --\u001b[0m\n",
      "Epoch 39: train loss = 0.07, val loss = 0.09, AUC = 1.00\u001b[31m --\u001b[0m\n",
      "Epoch 40: train loss = 0.07, val loss = 0.08, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 41: train loss = 0.07, val loss = 0.08, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 42: train loss = 0.07, val loss = 0.08, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 43: train loss = 0.06, val loss = 0.07, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 44: train loss = 0.06, val loss = 0.07, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 45: train loss = 0.06, val loss = 0.07, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 46: train loss = 0.06, val loss = 0.07, AUC = 1.00\u001b[31m --\u001b[0m\n",
      "Epoch 47: train loss = 0.06, val loss = 0.07, AUC = 1.00\u001b[31m --\u001b[0m\n",
      "Epoch 48: train loss = 0.06, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 49: train loss = 0.06, val loss = 0.09, AUC = 1.00\u001b[31m --\u001b[0m\n",
      "Epoch 50: train loss = 0.08, val loss = 0.13, AUC = 0.99\u001b[31m --\u001b[0m\n",
      "Epoch 51: train loss = 0.09, val loss = 0.09, AUC = 0.99\u001b[31m --\u001b[0m\n",
      "Epoch 52: train loss = 0.08, val loss = 0.08, AUC = 1.00\u001b[31m --\u001b[0m\n",
      "Epoch 53: train loss = 0.07, val loss = 0.08, AUC = 1.00\u001b[31m --\u001b[0m\n",
      "Epoch 54: train loss = 0.07, val loss = 0.07, AUC = 1.00\u001b[31m --\u001b[0m\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 55: train loss = 0.06, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 56: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 57: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 58: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 59: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 60: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 61: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 62: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 63: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 64: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 65: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 66: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 67: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 68: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 69: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 70: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 71: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 72: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 73: train loss = 0.05, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 74: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 75: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 76: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 77: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 78: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 79: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 80: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 81: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 82: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 83: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 84: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 85: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 86: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 87: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 88: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 89: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 90: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 91: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 92: train loss = 0.04, val loss = 0.06, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 93: train loss = 0.04, val loss = 0.05, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 94: train loss = 0.04, val loss = 0.05, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 95: train loss = 0.04, val loss = 0.05, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 96: train loss = 0.04, val loss = 0.05, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 97: train loss = 0.04, val loss = 0.05, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 98: train loss = 0.04, val loss = 0.05, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 99: train loss = 0.04, val loss = 0.05, AUC = 1.00\u001b[32m ++\u001b[0m\n",
      "Epoch 100: train loss = 0.04, val loss = 0.05, AUC = 1.00\u001b[32m ++\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeseriesClassifier(\n",
       "  (embedder): ResNet(\n",
       "    (resblock1): ResBlock(\n",
       "      (convblock1): ConvBlock(\n",
       "        (0): Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (convblock2): ConvBlock(\n",
       "        (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (convblock3): ConvBlock(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): ConvBlock(\n",
       "        (0): Conv1d(1, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add): Add\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (resblock2): ResBlock(\n",
       "      (convblock1): ConvBlock(\n",
       "        (0): Conv1d(64, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (convblock2): ConvBlock(\n",
       "        (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (convblock3): ConvBlock(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): ConvBlock(\n",
       "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add): Add\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (resblock3): ResBlock(\n",
       "      (convblock1): ConvBlock(\n",
       "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (convblock2): ConvBlock(\n",
       "        (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (convblock3): ConvBlock(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add): Add\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "    (squeeze): Squeeze(dim=-1)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (classifier): MultiLayerPerceptron(\n",
       "    (mappings): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Sequential(\n",
       "        (0): ReLU(inplace=True)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ReLU(inplace=True)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(data=data, classifier=classifier, n_epochs=100, batch_size=500, device=\"cuda\", learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-spice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
